{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKwFdIZxrBcePc+CkdQ/5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhavana0929/GenAI_course_practice/blob/main/NextWord_Prediction_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3tn0cEjEHqZ",
        "outputId": "c02f7181-1170-42f6-e27e-6cd9f99acce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "# Data Collection\n",
        "import nltk\n",
        "nltk.download(\"gutenberg\")\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
        "\n",
        "# Save to a file\n",
        "with open(\"hamlet.txt\",\"w\") as file:\n",
        "    file.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"hamlet.txt\",\"r\") as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index)+1\n",
        "total_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X_6KmioETUW",
        "outputId": "ffce4a24-a033-4ff8-d82d-951e3ced0b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4818"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "cj-C9FMhFWCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCUTUamxJLSs",
        "outputId": "5f5905a6-44c2-4d33-a0ed-31bbdec6fbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 687],\n",
              " [1, 687, 4],\n",
              " [1, 687, 4, 45],\n",
              " [1, 687, 4, 45, 41],\n",
              " [1, 687, 4, 45, 41, 1886],\n",
              " [1, 687, 4, 45, 41, 1886, 1887],\n",
              " [1, 687, 4, 45, 41, 1886, 1887, 1888],\n",
              " [1180, 1889],\n",
              " [1180, 1889, 1890],\n",
              " [1180, 1889, 1890, 1891]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad Sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding=\"pre\"))"
      ],
      "metadata": {
        "id": "IV5kNkcJKFjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDzOIIFTTmfz",
        "outputId": "339b87c7-11d9-4074-adb9-939698c3d625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    1,  687],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           1,  687,    4],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
              "         687,    4,   45],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  687,\n",
              "           4,   45,   41],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    1,  687,    4,\n",
              "          45,   41, 1886],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    1,  687,    4,   45,\n",
              "          41, 1886, 1887],\n",
              "       [   0,    0,    0,    0,    0,    0,    1,  687,    4,   45,   41,\n",
              "        1886, 1887, 1888],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0, 1180, 1889],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "        1180, 1889, 1890],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1180,\n",
              "        1889, 1890, 1891]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into labels and train data\n",
        "import tensorflow as tf\n",
        "x,y = input_sequences[:,:-1],input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "si_jzSBYT4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0El6vNCUn4l",
        "outputId": "d572eb36-b570-4217-9272-b3e1a8fa4ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test data\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "w4qfAOhzVKFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Early Stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\",patience=3,restore_best_weights=True)"
      ],
      "metadata": {
        "id": "dCKchrgjZHIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our LSTM RNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words,activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVNMqXz6Vv-E",
        "outputId": "81688ff0-dc01-485c-c34c-791b34cbee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loqX1eF_X5tl",
        "outputId": "2b4266ba-b495-4734-ffcd-c024b455770f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 66ms/step - accuracy: 0.0333 - loss: 7.1609 - val_accuracy: 0.0336 - val_loss: 6.6915\n",
            "Epoch 2/10\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.0378 - loss: 6.4659 - val_accuracy: 0.0375 - val_loss: 6.7611\n",
            "Epoch 3/10\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - accuracy: 0.0433 - loss: 6.3323 - val_accuracy: 0.0466 - val_loss: 6.8034\n",
            "Epoch 4/10\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 64ms/step - accuracy: 0.0517 - loss: 6.1977 - val_accuracy: 0.0515 - val_loss: 6.8163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "def predict_next_word(model,tokenizer,text,max_sequence_len):\n",
        "  token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "  if len(token_list) >= max_sequence_len:\n",
        "    token_list = token_list[-(max_sequence_len-1):]\n",
        "  token_list = pad_sequences([token_list],maxlen=max_sequence_len-1,padding=\"pre\")\n",
        "  predicted = model.predict(token_list,verbose=0)\n",
        "  predicted_word_index = np.argmax(predicted,axis=1)\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == predicted_word_index:\n",
        "      return word\n",
        "  return None"
      ],
      "metadata": {
        "id": "i-DdeFwXZvnL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word = predict_next_word(model,tokenizer,\"To be or not to be\",max_sequence_len)"
      ],
      "metadata": {
        "id": "cc9GifUqb0B1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word"
      ],
      "metadata": {
        "id": "SJ2GE4Z6cRu9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1860c1e1-d8f8-49e0-91f0-2ebec79532db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the model\n",
        "model.save(\"next_word_lstm.h5\")\n",
        "\n",
        "# Save the tokenizer\n",
        "with open(\"tokenizer.pickle\",\"wb\") as file:\n",
        "  pickle.dump(tokenizer,file,protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "a3-Ut6mSeCnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e8d0e2-8683-4432-dfdc-b4f31f9dd8c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA_i4Z5zy_kZ",
        "outputId": "d3415200-f293-4a74-be0b-7efae786e099"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.43.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.30.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "--2025-03-17 20:34:12--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.2.1/cloudflared-linux-amd64 [following]\n",
            "--2025-03-17 20:34:12--  https://github.com/cloudflare/cloudflared/releases/download/2025.2.1/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/eac8237f-c554-46b5-95ea-f2f5873e69a5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250317%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250317T203219Z&X-Amz-Expires=300&X-Amz-Signature=639c60e1025f81bd1d74baffeca1642df4257951999a07de3f76459b7abce3c1&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-17 20:34:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/eac8237f-c554-46b5-95ea-f2f5873e69a5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250317%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250317T203219Z&X-Amz-Expires=300&X-Amz-Signature=639c60e1025f81bd1d74baffeca1642df4257951999a07de3f76459b7abce3c1&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37844205 (36M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared’\n",
            "\n",
            "cloudflared         100%[===================>]  36.09M   184MB/s    in 0.2s    \n",
            "\n",
            "2025-03-17 20:34:12 (184 MB/s) - ‘cloudflared’ saved [37844205/37844205]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Load the LSTM Model\n",
        "model=load_model('next_word_lstm.keras')\n",
        "\n",
        "#3 Load the tokenizer\n",
        "with open('tokenizer.pkl','rb') as handle:\n",
        "    tokenizer=pickle.load(handle)\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    if len(token_list) >= max_sequence_len:\n",
        "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# streamlit app\n",
        "st.title(\"Next Word Prediction With LSTM And Early Stopping\")\n",
        "input_text=st.text_input(\"Enter the sequence of Words\",\"To be or not to\")\n",
        "if st.button(\"Predict Next Word\"):\n",
        "    max_sequence_len = model.input_shape[1] + 1  # Retrieve the max sequence length from the model input shape\n",
        "    next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
        "    st.write(f'Next word: {next_word}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QOzLSLGfCTa",
        "outputId": "b891d2aa-0d72-4709-ee7d-4517cfd2c4f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "L5pfLqH5f8fC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "id": "KGY8W2S5gG7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc12510-28c1-4d4b-9222-f894179cb873"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-03-17T20:38:30Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-03-17T20:38:30Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m |  https://maritime-subsidiaries-ol-nation.trycloudflare.com                                 |\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.2.1 (Checksum afdfadd1ef552e66bffc35246fe30a9bd578356d2d386de95585ccfc432472b8)\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.22.10, GoArch: amd64\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/run-tunnel/as-a-service/\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 59f4deb4-4652-4930-9b22-74c5ff778a44\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Using [CurveID(4588) CurveID(25497) CurveP256] as curve preferences \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113\n",
            "2025/03/17 20:38:34 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-03-17T20:38:34Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0md3d06994-ccf0-4a1d-8da4-34cacf93b901 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113 \u001b[36mlocation=\u001b[0matl11 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-03-17T20:38:51Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 45 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-03-17T20:38:51Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 45 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://maritime-subsidiaries-ol-nation.trycloudflare.com/static/js/index.DmrHunjf.js \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-03-17T20:38:54Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 61 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2025-03-17T20:38:54Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 61 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://maritime-subsidiaries-ol-nation.trycloudflare.com/static/js/index.DmrHunjf.js \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2025-03-17T20:45:27Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-03-17T20:45:28Z\u001b[0m \u001b[32mINF\u001b[0m Unregistered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113\n",
            "\u001b[90m2025-03-17T20:45:28Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113\n",
            "\u001b[90m2025-03-17T20:45:28Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.113\n",
            "\u001b[90m2025-03-17T20:45:28Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-03-17T20:45:28Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-03-17T20:45:28Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    }
  ]
}